Best preprocessor:
{'count__stop_words': None, 'count__max_df': 0.8, 'lda': LDAVectorizer(V=10000, alpha=0.01, kappa=0.5,
       lda_model=OnlineLDAVB(K=50, V=10000, alpha=0.01, batch_size=256,
      beta=array([[  2.36855e-06,   7.97642e-06, ...,   8.60111e-07,   9.82286e-07],
       [  1.16481e-06,   2.52089e-06, ...,   6.51328e-07,   1.00379e-06],
       ...,
       [  1.37293e-05,   1.38633e-05, ...,   7.47071e-07,   5.82075e-07],
       [  1.57732e-06,   1.02242e-06, ...,   1.08697e-06,   1.69710e-04]]),
      kappa=0.5, predictive_ratio=0.8, t=45, tau0=64, var_converged=1e-06,
      var_max_iter=100),
       num_topics=50, perplexity=False, size=256, tau0=64, var_i=100), 'count__decode_error': u'strict', 'lda__V': 10000, 'lda__num_topics': 50, 'lda__size': 256, 'count__lowercase': True, 'count__max_features': 10000, 'count__input': u'content', 'lda__lda_model': OnlineLDAVB(K=50, V=10000, alpha=0.01, batch_size=256,
      beta=array([[  2.36855e-06,   7.97642e-06, ...,   8.60111e-07,   9.82286e-07],
       [  1.16481e-06,   2.52089e-06, ...,   6.51328e-07,   1.00379e-06],
       ...,
       [  1.37293e-05,   1.38633e-05, ...,   7.47071e-07,   5.82075e-07],
       [  1.57732e-06,   1.02242e-06, ...,   1.08697e-06,   1.69710e-04]]),
      kappa=0.5, predictive_ratio=0.8, t=45, tau0=64, var_converged=1e-06,
      var_max_iter=100), 'count__preprocessor': None, 'count__analyzer': u'word', 'count__min_df': 3, 'count__vocabulary': None, 'lda__kappa': 0.5, 'count__token_pattern': u'(?u)\\b\\w\\w+\\b', 'count__encoding': u'utf-8', 'lda__tau0': 64, 'count__binary': False, 'lda__perplexity': False, 'count__strip_accents': None, 'memory': None, 'count__ngram_range': (1, 3), 'lda__alpha': 0.01, 'count': CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.8, max_features=10000, min_df=3,
        ngram_range=(1, 3), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'count__tokenizer': None, 'lda__var_i': 100, 'count__dtype': <type 'numpy.int64'>, 'steps': [('count', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.8, max_features=10000, min_df=3,
        ngram_range=(1, 3), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)), ('lda', LDAVectorizer(V=10000, alpha=0.01, kappa=0.5,
       lda_model=OnlineLDAVB(K=50, V=10000, alpha=0.01, batch_size=256,
      beta=array([[  2.36855e-06,   7.97642e-06, ...,   8.60111e-07,   9.82286e-07],
       [  1.16481e-06,   2.52089e-06, ...,   6.51328e-07,   1.00379e-06],
       ...,
       [  1.37293e-05,   1.38633e-05, ...,   7.47071e-07,   5.82075e-07],
       [  1.57732e-06,   1.02242e-06, ...,   1.08697e-06,   1.69710e-04]]),
      kappa=0.5, predictive_ratio=0.8, t=45, tau0=64, var_converged=1e-06,
      var_max_iter=100),
       num_topics=50, perplexity=False, size=256, tau0=64, var_i=100))]}


Best classifier:
{'loss': 'hinge', 'C': 0.01, 'verbose': 0, 'intercept_scaling': 1, 'fit_intercept': True, 'max_iter': 500, 'penalty': 'l2', 'multi_class': 'ovr', 'random_state': None, 'dual': True, 'tol': 0.0001, 'class_weight': None}


             precision    recall  f1-score   support

          0       0.59      0.39      0.47       319
          1       0.37      0.22      0.27       389
          2       0.34      0.22      0.27       394
          3       0.39      0.31      0.35       392
          4       0.26      0.36      0.30       385
          5       0.54      0.71      0.62       395
          6       0.61      0.78      0.68       390
          7       0.74      0.69      0.71       396
          8       0.69      0.80      0.74       398
          9       0.81      0.82      0.82       397
         10       0.85      0.92      0.89       399
         11       0.82      0.89      0.85       396
         12       0.40      0.36      0.38       393
         13       0.64      0.65      0.64       396
         14       0.74      0.84      0.78       394
         15       0.61      0.84      0.70       398
         16       0.62      0.66      0.64       364
         17       0.87      0.81      0.84       376
         18       0.59      0.47      0.52       310
         19       0.40      0.22      0.29       251

avg / total       0.60      0.61      0.60      7532



Predict time: 0.007038
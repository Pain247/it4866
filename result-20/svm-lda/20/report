Best preprocessor:
{'count__stop_words': None, 'count__max_df': 0.8, 'lda': LDAVectorizer(V=10000, alpha=0.05, kappa=0.5,
       lda_model=OnlineLDAVB(K=20, V=10000, alpha=0.05, batch_size=256,
      beta=array([[  5.39876e-06,   5.89170e-06, ...,   7.03696e-07,   1.40832e-06],
       [  1.09895e-06,   2.69325e-06, ...,   6.13882e-07,   6.70392e-07],
       ...,
       [  9.97864e-07,   5.51899e-07, ...,   6.98202e-07,   5.72433e-07],
       [  1.51915e-06,   2.73865e-06, ...,   3.13366e-04,   4.88779e-05]]),
      kappa=0.5, predictive_ratio=0.8, t=45, tau0=64, var_converged=1e-06,
      var_max_iter=100),
       num_topics=20, perplexity=False, size=256, tau0=64, var_i=100), 'count__decode_error': u'strict', 'lda__V': 10000, 'lda__num_topics': 20, 'lda__size': 256, 'count__lowercase': True, 'count__max_features': 10000, 'count__input': u'content', 'lda__lda_model': OnlineLDAVB(K=20, V=10000, alpha=0.05, batch_size=256,
      beta=array([[  5.39876e-06,   5.89170e-06, ...,   7.03696e-07,   1.40832e-06],
       [  1.09895e-06,   2.69325e-06, ...,   6.13882e-07,   6.70392e-07],
       ...,
       [  9.97864e-07,   5.51899e-07, ...,   6.98202e-07,   5.72433e-07],
       [  1.51915e-06,   2.73865e-06, ...,   3.13366e-04,   4.88779e-05]]),
      kappa=0.5, predictive_ratio=0.8, t=45, tau0=64, var_converged=1e-06,
      var_max_iter=100), 'count__preprocessor': None, 'count__analyzer': u'word', 'count__min_df': 3, 'count__vocabulary': None, 'lda__kappa': 0.5, 'count__token_pattern': u'(?u)\\b\\w\\w+\\b', 'count__encoding': u'utf-8', 'lda__tau0': 64, 'count__binary': False, 'lda__perplexity': False, 'count__strip_accents': None, 'memory': None, 'count__ngram_range': (1, 3), 'lda__alpha': 0.05, 'count': CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.8, max_features=10000, min_df=3,
        ngram_range=(1, 3), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'count__tokenizer': None, 'lda__var_i': 100, 'count__dtype': <type 'numpy.int64'>, 'steps': [('count', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',
        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
        lowercase=True, max_df=0.8, max_features=10000, min_df=3,
        ngram_range=(1, 3), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)), ('lda', LDAVectorizer(V=10000, alpha=0.05, kappa=0.5,
       lda_model=OnlineLDAVB(K=20, V=10000, alpha=0.05, batch_size=256,
      beta=array([[  5.39876e-06,   5.89170e-06, ...,   7.03696e-07,   1.40832e-06],
       [  1.09895e-06,   2.69325e-06, ...,   6.13882e-07,   6.70392e-07],
       ...,
       [  9.97864e-07,   5.51899e-07, ...,   6.98202e-07,   5.72433e-07],
       [  1.51915e-06,   2.73865e-06, ...,   3.13366e-04,   4.88779e-05]]),
      kappa=0.5, predictive_ratio=0.8, t=45, tau0=64, var_converged=1e-06,
      var_max_iter=100),
       num_topics=20, perplexity=False, size=256, tau0=64, var_i=100))]}


Best classifier:
{'loss': 'hinge', 'C': 0.040000000000000008, 'verbose': 0, 'intercept_scaling': 1, 'fit_intercept': True, 'max_iter': 500, 'penalty': 'l2', 'multi_class': 'ovr', 'random_state': None, 'dual': True, 'tol': 0.0001, 'class_weight': None}


             precision    recall  f1-score   support

          0       0.33      0.05      0.09       319
          1       0.08      0.04      0.05       389
          2       0.26      0.36      0.30       394
          3       0.30      0.44      0.36       392
          4       0.24      0.20      0.22       385
          5       0.40      0.52      0.45       395
          6       0.61      0.39      0.48       390
          7       0.53      0.65      0.58       396
          8       0.37      0.78      0.50       398
          9       0.26      0.21      0.23       397
         10       0.59      0.30      0.40       399
         11       0.70      0.71      0.70       396
         12       0.28      0.08      0.13       393
         13       0.38      0.08      0.13       396
         14       0.67      0.69      0.68       394
         15       0.59      0.54      0.56       398
         16       0.59      0.30      0.40       364
         17       0.63      0.67      0.65       376
         18       0.11      0.45      0.18       310
         19       0.05      0.01      0.01       251

avg / total       0.41      0.38      0.37      7532



Predict time: 0.005911